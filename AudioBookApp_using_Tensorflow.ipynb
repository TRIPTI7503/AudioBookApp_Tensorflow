{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Extract the data from the csv"
      ],
      "metadata": {
        "id": "GMYqpf3ETae0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBDW6hCtploT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "raw_csv_data = np.loadtxt(\"/content/Audiobooks_data.csv\", delimiter = ',')\n",
        "\n",
        "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
        "targets_all = raw_csv_data[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balance the data"
      ],
      "metadata": {
        "id": "JgCMcg_dT1Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_one_targets = int(np.sum(targets_all))\n",
        "zero_targets_counter = 0\n",
        "indices_to_remove = []\n",
        "\n",
        "for i in range(targets_all.shape[0]):\n",
        "  if targets_all[i] == 0:\n",
        "      zero_targets_counter +=1\n",
        "      if zero_targets_counter > num_one_targets:\n",
        "        indices_to_remove.append(i)\n",
        "\n",
        "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
        "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)"
      ],
      "metadata": {
        "id": "NaTI0iFoTgKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize the inputs"
      ],
      "metadata": {
        "id": "z2TjFGUqgxYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
      ],
      "metadata": {
        "id": "_V7beUEYVKx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle the data"
      ],
      "metadata": {
        "id": "B9Mp_trJhArW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]"
      ],
      "metadata": {
        "id": "9YJdwkL9g_SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into train, validation and test"
      ],
      "metadata": {
        "id": "4mOFf0_tn-Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples_count = shuffled_inputs.shape[0]\n",
        "\n",
        "train_samples_count = int(0.8 * samples_count)\n",
        "validation_samples_count = int(0.1 * samples_count)\n",
        "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "train_inputs = shuffled_inputs[:train_samples_count]\n",
        "train_targets = shuffled_targets[:train_samples_count]\n",
        "\n",
        "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
        "\n",
        "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
        "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
        "\n",
        "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
        "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
        "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXL05zsDn852",
        "outputId": "22473cc0-fc08-4b7a-888c-5a112e9d2b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1786.0 3579 0.4990220732048058\n",
            "222.0 447 0.4966442953020134\n",
            "229.0 448 0.5111607142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the three datasets in *.npz"
      ],
      "metadata": {
        "id": "IzIl7tEHxudq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('/content/Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('/content/Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('/content/Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "metadata": {
        "id": "uUeJqDhXxO45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data"
      ],
      "metadata": {
        "id": "hEhJ0SL6RVAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npz = np.load('Audiobooks_data_train.npz')\n",
        "\n",
        "train_inputs = npz['inputs'].astype(float)\n",
        "train_targets = npz['targets'].astype(int)\n",
        "\n",
        "npz = np.load('Audiobooks_data_validation.npz')\n",
        "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
        "\n",
        "npz = np.load('Audiobooks_data_test.npz')\n",
        "test_inputs, test_targets = npz['inputs'].astype(float), npz['targets'].astype(int)"
      ],
      "metadata": {
        "id": "5P86nZqIxt5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "y5jjbdzkTqum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 10\n",
        "output_size = 2\n",
        "hidden_layer_size = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "max_epochs = 100\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model.fit(train_inputs,\n",
        "          train_targets,\n",
        "          batch_size=batch_size,\n",
        "          epochs=max_epochs,\n",
        "          callbacks = [early_stopping],\n",
        "          validation_data=(validation_inputs, validation_targets),\n",
        "          verbose = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoL0jFnERWA0",
        "outputId": "16d05bd0-44f9-47ee-c5a9-e36300b5c8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 - 2s - loss: 0.5337 - accuracy: 0.7279 - val_loss: 0.4579 - val_accuracy: 0.7830 - 2s/epoch - 50ms/step\n",
            "Epoch 2/100\n",
            "36/36 - 0s - loss: 0.4079 - accuracy: 0.7837 - val_loss: 0.4011 - val_accuracy: 0.7897 - 171ms/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "36/36 - 0s - loss: 0.3718 - accuracy: 0.8030 - val_loss: 0.3879 - val_accuracy: 0.8054 - 146ms/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "36/36 - 0s - loss: 0.3575 - accuracy: 0.8083 - val_loss: 0.3790 - val_accuracy: 0.7875 - 141ms/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "36/36 - 0s - loss: 0.3500 - accuracy: 0.8125 - val_loss: 0.3661 - val_accuracy: 0.8121 - 164ms/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "36/36 - 0s - loss: 0.3427 - accuracy: 0.8156 - val_loss: 0.3615 - val_accuracy: 0.8009 - 158ms/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "36/36 - 0s - loss: 0.3373 - accuracy: 0.8192 - val_loss: 0.3644 - val_accuracy: 0.7942 - 154ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "36/36 - 0s - loss: 0.3371 - accuracy: 0.8145 - val_loss: 0.3656 - val_accuracy: 0.8098 - 167ms/epoch - 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f27b10e56c0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model"
      ],
      "metadata": {
        "id": "tKhuSeAR--Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEpjHc14UFAq",
        "outputId": "f090a525-32aa-4fd4-af3d-091622643954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test loss: 0.33. Test accuracy: 79.69%\n"
          ]
        }
      ]
    }
  ]
}